#+TITLE: March 2017 Articles

* DONE [[http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation][Damn Cool Algorithms: Cardinality Estimation]]             :cardinality:
  CLOSED: [2017-10-02 Mon 16:18]
  - A simple and intuitive cardinality estimator
    Suppose:
    1. Generate n evenly distributed random numbers
    2. Arbitrarily replicate some of them an unspecified number of times
    3. Shuffle the set of numbers arbitrarily
    - Estimate the number of unique numbers?
  - If we know the set was evenly distributed, we can simply find the
    smallest number in the set.  If the maximum *possible* value is
    =m= and the smallest we find is ==, we can estimate =m/x= values
    in the total set.  This isn't very accurate, of course, but is
    definitely straightforward.

    1, 2, 3, 4, 5, 5, 5, 6, 10 = > 20 / 1 => 20 => actual: 7
    3, 4 6, 6, 6, 6, 6, 10 => 20 / 3 => 6ish => actual: 4
  -  "Probabilistic Counting Algorithms for Data Base Applications by
     Flajolet and Martin, with further refinements in the papers
     LogLog counting of large cardinalities by Durand-Flajolet, and
     HyperLogLog: The analysis of a near-optimal cardinality
     estimation algorithm by Flajolet et al."
  - A good hash function can turn any set of data into the type we need.
  - Other metrics exist that aren't the *minimum* value.
    - Flajolet and Martin pick the number of 0 bits at the beginning of the hashed values.
      - A sequence of =k= zero bits will occur once in every =2^k= elements, on average.
      - Record the length of the *longest* sequence to estimate the total number of unique elements.
      - The best we can get, though, is a power of 2, i.e., huge variance.
      - Estimate is small: recording sequences of leading 0s up to 32 bits, we only need a 5 bit number.
    - How can we improve upon that?
      - One idea, use multiple independent hash functions. Average the longest runs of 0s
      - Hashing is expensive, but result is pretty good.
      - Use a single hash function, take N bits, and use the leading 0s of the other 22bits (32-bit hash function)
      - This is essentially the LogLog algorithm from Durand-Flajolet paper.
        - =2 ** (float(sum(max_zeroes)) / num_buckets) * num_buckets * 0.79402=
        - The magic number corrects for some bias and was derived in the paper.
        - Average error is =1.3/sqrt(m)=, about 4% for 1024 buckets
          - 5 bits per bucket is enough to estimate cardinalities up to 227 per the paper
  - Improving accuracy: SuperLogLog and HyperLogLog
    - Durand and Flajolet make the observation that outlying values do
      a lot to decrease the accuracy of the estimate; by throwing out
      the largest values before averaging, accuracy can be improved.
    - Throw out 30% of the buckets with the largest values and
      averaging only 70% of buckets with smaller values, sees an
      improvement from =1.3/sqrt(m)= to =1.05/sqrt(m)=!. For 1024
      buckets, 3.2%!
    - Finally, the major contribution of Flajolet et al in the
      HyperLogLog paper is to use a different type of averaging,
      taking the harmonic mean instead of the geometric mean we just
      applied. By doing this, they're able to edge down the error to
      1.04/sqrt(m), again with no increase in state required.
  - Easy to parallelize. Multiple machines can independently run the
    algorithm with same hash function / buckets At the end, results
    can be combined by taking maximum value of each bucket from each
    instance of the algorithm.
